{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b34dd5",
   "metadata": {},
   "source": [
    "# Step 3-3 Multivariate DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939be8",
   "metadata": {},
   "source": [
    "Multivariate Deep Learning models.\n",
    "\n",
    "The models are multivariate versions from the baseline models: LSTM, Bi-directional LSTM, ED-LSTM, CNN\n",
    "\n",
    "**Data prep and train workflow**:\n",
    "\n",
    "raw data -> split train and test -> pct_change normalisation -> combine all countries' train data -> fit universal StandardScaler\n",
    "\n",
    "-> create sequences (5 input windows, 1 output window) -> train models with hyperparmeter tuning\n",
    "\n",
    "**Test and evaluation workflow**:\n",
    "\n",
    "for each country, combine train + test data (lags) -> calculate pct_change -> scale with the previous Scaler -> create sequences\n",
    "\n",
    "-> Extract the last 9 sequences -> Model prediction -> Inverse StandardScaler -> Denormalise -> calculate metrics\n",
    "\n",
    "The selected features from the previous feature selection step:\n",
    "\n",
    "* Key features = `gdp`, `primary_energy_consumption`, `population`\n",
    "\n",
    "* Selected features = `oil_production`, `nulcear_consumption`, `wind_consumption`, `biofuel_consumption`, `energy_per_gdp`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7ff0c",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186690ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Conv1D, Flatten\n",
    "from keras.layers import RepeatVector, TimeDistributed, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a074bd",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dffb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VARIABLES = 'co2'\n",
    "SELECTED_COUNTRIES = ['United States', 'China', 'India']\n",
    "MAX_LAGS = 4\n",
    "N_STEPS_IN = 5\n",
    "N_STEPS_OUT = 1\n",
    "TEST_SIZE = 9\n",
    "save_dir = 'data/03_03_results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "MULTIVARIATE_FEATURES = [\n",
    "    'gdp',\n",
    "    'primary_energy_consumption',\n",
    "    'population',\n",
    "    'oil_production',\n",
    "    'nuclear_consumption',\n",
    "    'wind_consumption',\n",
    "    'biofuel_consumption',\n",
    "    'energy_per_gdp'\n",
    "]\n",
    "\n",
    "ARIMA_ORDERS = {\n",
    "    'United States': (0, 1, 0),\n",
    "    'China': (0, 2, 0),\n",
    "    'India': (1, 1, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4129d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca6d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_CONFIGS = {\n",
    "    'LSTM': [\n",
    "        {'hidden': 16, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 32, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 64, 'epochs': 100, 'dropout': 0.2}\n",
    "    ],\n",
    "    'Bi-LSTM': [\n",
    "        {'hidden': 8, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 16, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 32, 'epochs': 100, 'dropout': 0.2}\n",
    "    ],\n",
    "    'ED-LSTM': [\n",
    "        {'hidden': 8, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 16, 'epochs': 100, 'dropout': 0.0},\n",
    "        {'hidden': 32, 'epochs': 100, 'dropout': 0.2}\n",
    "    ],\n",
    "    'CNN': [\n",
    "        {'filters': 16, 'epochs': 100},\n",
    "        {'filters': 32, 'epochs': 100},\n",
    "        {'filters': 64, 'epochs': 100}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a368c5",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a93e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(save_dir='data'):\n",
    "    data_files = {\n",
    "        'all_data_df': os.path.join(save_dir, 'all_data_df.csv'),\n",
    "        'g20_lag_df': os.path.join(save_dir, 'g20_lag_df.csv'),\n",
    "        'lag_three_sel_1969_df': os.path.join(save_dir, 'lag_three_sel_1969_df.csv')\n",
    "    }\n",
    "\n",
    "    dfs = {}\n",
    "    for name, filepath in data_files.items():\n",
    "        if os.path.exists(filepath):\n",
    "            dfs[name] = pd.read_csv(filepath)\n",
    "            print(f\"Loaded {name}: {dfs[name].shape}\")\n",
    "        else:\n",
    "            print(f\"{filepath} not found\")\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f1f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all_data_df: (55529, 200)\n",
      "Loaded g20_lag_df: (3744, 992)\n",
      "Loaded lag_three_sel_1969_df: (162, 992)\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "all_data_df = data['all_data_df']\n",
    "g20_lag_df = data['g20_lag_df']\n",
    "g20_lag_1969_df = g20_lag_df[g20_lag_df['year'] >= 1969].copy()\n",
    "g20_lag_1969_df = g20_lag_1969_df[g20_lag_1969_df['year'] < 2023]\n",
    "lag_three_sel_1969_df = data['lag_three_sel_1969_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b770f",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6190ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts_by_year(df, test_size=9):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for country in df['country'].unique():\n",
    "        country_data = df[df['country'] == country].sort_values('year')\n",
    "\n",
    "        split_idx = len(country_data) - test_size\n",
    "        train_data[country] = country_data.iloc[:split_idx]\n",
    "        test_data[country] = country_data.iloc[split_idx:]\n",
    "\n",
    "    train_df = pd.concat(train_data.values(), ignore_index=True)\n",
    "    test_df = pd.concat(test_data.values(), ignore_index=True)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e5a7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (135, 992)\n",
      "Test data shape: (27, 992)\n"
     ]
    }
   ],
   "source": [
    "train_3_df, test_3_df = tts_by_year(lag_three_sel_1969_df, TEST_SIZE)\n",
    "train_g20_df, test_g20_df = tts_by_year(g20_lag_1969_df, TEST_SIZE)\n",
    "\n",
    "print(f\"Train data shape: {train_3_df.shape}\")\n",
    "print(f\"Test data shape: {test_3_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f22e2",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0e6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase(y_actual, y_pred, period=1):\n",
    "    mae_forecast = mean_absolute_error(y_actual, y_pred)\n",
    "\n",
    "    naive_forecast = y_actual[:-period] if period > 0 else y_actual[:-1]\n",
    "    actual_for_naive = y_actual[period:] if period > 0 else y_actual[1:]\n",
    "\n",
    "    if len(naive_forecast) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    mae_naive = mean_absolute_error(actual_for_naive, naive_forecast)\n",
    "\n",
    "    if mae_naive == 0:\n",
    "        return 0 if mae_forecast == 0 else np.inf\n",
    "    \n",
    "    return mae_forecast / mae_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "555b40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pct_change(df, features, max_lags=MAX_LAGS):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sort_values(['country', 'year']).reset_index(drop=True)\n",
    "    pct_change_cols = []\n",
    "\n",
    "    for feature in features:\n",
    "        if feature not in df_copy.columns:\n",
    "            continue\n",
    "\n",
    "        # Pct change on current values\n",
    "        lag1_col = f\"{feature}_lag1\"\n",
    "        if lag1_col in df_copy.columns:\n",
    "            df_copy[f\"{feature}_pct_change\"] = ((df_copy[feature] - df_copy[lag1_col]) / df_copy[lag1_col] * 100)\n",
    "            pct_change_cols.append(f\"{feature}_pct_change\")\n",
    "\n",
    "        # Pct change on lagged values\n",
    "        for lag in range(1, max_lags):\n",
    "            lag_col = f\"{feature}_lag{lag}\"\n",
    "            prev_lag_col = f\"{feature}_lag{lag+1}\"\n",
    "            \n",
    "            if lag_col in df_copy.columns and prev_lag_col in df_copy.columns:\n",
    "                df_copy[f\"{lag_col}_pct_change\"] = ((df_copy[lag_col] - df_copy[prev_lag_col]) / df_copy[prev_lag_col] * 100)\n",
    "                pct_change_cols.append(f\"{lag_col}_pct_change\")\n",
    "\n",
    "        # Lag4 for the first row = 0, then shift lag3_pct by country\n",
    "        last_lag_col = f\"{feature}_lag{max_lags}\"\n",
    "        lag3_pct_col = f\"{feature}_lag{max_lags-1}_pct_change\"\n",
    "\n",
    "        if last_lag_col in df_copy.columns and lag3_pct_col in df_copy.columns:\n",
    "            df_copy[f\"{last_lag_col}_pct_change\"] = df_copy.groupby('country')[lag3_pct_col].shift(1).fillna(0)\n",
    "            pct_change_cols.append(f\"{last_lag_col}_pct_change\")\n",
    "            \n",
    "        df_copy = df_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df_copy, pct_change_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d974164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps_in, n_features):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - n_steps_in):\n",
    "        X.append(data[i:i + n_steps_in])\n",
    "        y.append(data[i + n_steps_in, 0]) # Target co2 is the first column\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ae5eb",
   "metadata": {},
   "source": [
    "### Model Builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187f2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(input_shape, hidden=16, dropout=0.0):\n",
    "    model = Sequential([\n",
    "        LSTM(hidden, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(dropout),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083d1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm(input_shape, hidden=8, dropout=0.0):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(hidden, activation='relu', kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.01)),\n",
    "                        input_shape=input_shape),\n",
    "        Dropout(dropout),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4032ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edlstm(input_shape, hidden=8, dropout=0.0):\n",
    "    model = Sequential([\n",
    "        LSTM(hidden, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(dropout),\n",
    "        RepeatVector(N_STEPS_OUT),\n",
    "        LSTM(hidden, activation='relu', return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        TimeDistributed(Dense(1))\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6037d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, filters=16):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=input_shape, padding='same', kernel_regularizer=l2(0.01)),\n",
    "        Flatten(),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833dbb8e",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1f0f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "def prepare_train_data(train_df, features, target):\n",
    "    all_train_pct_data = []\n",
    "    country_train_info = {}\n",
    "\n",
    "    for country in train_df['country'].unique():\n",
    "        country_data = train_df[train_df['country'] == country].sort_values('year').reset_index(drop=True)\n",
    "\n",
    "        # Caculate pct_change for all features\n",
    "        country_pct, pct_cols = calculate_pct_change(country_data, features, MAX_LAGS)\n",
    "\n",
    "        # pct_change values for all features\n",
    "        feature_pct_values = []\n",
    "        for feat in features:\n",
    "            pct_col = f\"{feat}_pct_change\"\n",
    "            if pct_col in country_pct.columns:\n",
    "                pct_values = country_pct[pct_col].fillna(0).values\n",
    "                feature_pct_values.append(pct_values)\n",
    "            else:\n",
    "                print(f\"{pct_col} not in {country}\")\n",
    "        \n",
    "        # Stacking features: shape (timesteps, n_features)\n",
    "        feature_pct_array = np.column_stack(feature_pct_values)\n",
    "        all_train_pct_data.append(feature_pct_array)\n",
    "        \n",
    "        country_train_info[country] = {\n",
    "            'pct_values': feature_pct_array,\n",
    "            'original_values': country_data[target].values,\n",
    "            'years': country_data['year'].values\n",
    "        }\n",
    "        \n",
    "        print(f\"{country} pct_values shape: {feature_pct_array.shape}\")\n",
    "    \n",
    "    return all_train_pct_data, country_train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc25093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China pct_values shape: (45, 8)\n",
      "India pct_values shape: (45, 8)\n",
      "United States pct_values shape: (45, 8)\n"
     ]
    }
   ],
   "source": [
    "all_train_pct_data, country_train_info = prepare_train_data(train_3_df, MULTIVARIATE_FEATURES, TARGET_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c74c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit universal StandardScaler\n",
    "def fit_scaler(all_train_pct_data, features):\n",
    "    all_train_pct_combined = np.vstack(all_train_pct_data)\n",
    "    print(f\"Combined train data shape: {all_train_pct_combined.shape}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_train_pct_combined)\n",
    "\n",
    "    for i, feat in enumerate(features):\n",
    "        print(f\"{feat} mean={scaler.mean_[i]:.4f}, std={np.sqrt(scaler.var_[i]):.4f}\")\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdfd5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train data shape: (135, 8)\n",
      "gdp mean=4.8601, std=3.1878\n",
      "primary_energy_consumption mean=4.5367, std=4.6948\n",
      "population mean=1.4430, std=0.5910\n",
      "oil_production mean=3.7511, std=9.7537\n",
      "nuclear_consumption mean=15.2010, std=72.4845\n",
      "wind_consumption mean=2225.8822, std=25395.8016\n",
      "biofuel_consumption mean=7.5562, std=22.1054\n",
      "energy_per_gdp mean=-0.2989, std=3.5189\n"
     ]
    }
   ],
   "source": [
    "scaler = fit_scaler(all_train_pct_data, MULTIVARIATE_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1713cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_sequences(country_train_info, scaler, features):\n",
    "    X_train_all = []\n",
    "    y_train_all = []\n",
    "\n",
    "    for country, info in country_train_info.items():\n",
    "        pct_values = info['pct_values']\n",
    "        scaled_values = scaler.transform(pct_values)\n",
    "\n",
    "        X_country, y_country = create_sequences(scaled_values, N_STEPS_IN, len(features))\n",
    "\n",
    "        X_train_all.append(X_country)\n",
    "        y_train_all.append(y_country)\n",
    "\n",
    "        print(f\"{country} X_train shape: {X_country.shape}, y_train shape: {y_country.shape}\")\n",
    "    \n",
    "    X_train = np.vstack(X_train_all)\n",
    "    y_train = np.concatenate(y_train_all)\n",
    "    \n",
    "    print(f\"\\nCombined training sequences:\")\n",
    "    print(f\"    X_train shape: {X_train.shape}\")\n",
    "    print(f\"    y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22991f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China X_train shape: (40, 5, 8), y_train shape: (40,)\n",
      "India X_train shape: (40, 5, 8), y_train shape: (40,)\n",
      "United States X_train shape: (40, 5, 8), y_train shape: (40,)\n",
      "\n",
      "Combined training sequences:\n",
      "    X_train shape: (120, 5, 8)\n",
      "    y_train shape: (120,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_train_sequences(country_train_info, scaler, MULTIVARIATE_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42f40e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and Tune hyperparameters\n",
    "def train_model_and_tuning(X_train, y_train):\n",
    "    dl_models = {\n",
    "        'LSTM': build_lstm,\n",
    "        'Bi-LSTM': build_bilstm,\n",
    "        'ED-LSTM': build_edlstm,\n",
    "        'CNN': build_cnn\n",
    "    }\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "    for model_name, model_func in dl_models.items():\n",
    "        print(f\"\\nTraining {model_name}\")\n",
    "\n",
    "        best_val_loss = np.inf\n",
    "        best_model = None\n",
    "        best_config = None\n",
    "\n",
    "        # Reshape y_train for ED-LSTM output shape (n_samples, n_steps_out, n_features)\n",
    "        if model_name == 'ED-LSTM':\n",
    "            y_train_model = y_train.reshape(-1, N_STEPS_OUT, 1)\n",
    "            print(f\"ED-LSTM target shape: {y_train_model.shape}\")\n",
    "        else:\n",
    "            y_train_model = y_train\n",
    "            print(f\"{model_name} target shape: {y_train_model.shape}\")\n",
    "\n",
    "        for config in DL_CONFIGS[model_name]:\n",
    "            epochs = config['epochs']\n",
    "            config_params = {k: v for k, v in config.items() if k != 'epochs'}\n",
    "\n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "            if model_name == 'CNN':\n",
    "                model = model_func(input_shape, filters=config_params['filters'])\n",
    "            else:\n",
    "                model = model_func(input_shape, hidden=config_params['hidden'], dropout=config_params.get('dropout', 0.0))\n",
    "\n",
    "            history = model.fit(X_train, y_train_model, epochs=epochs, batch_size=16, validation_split=0.1, verbose=0)\n",
    "\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            print(f\"    For config: {config}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                best_config = config\n",
    "            else:\n",
    "                tf.keras.backend.clear_session()\n",
    "                del model\n",
    "        \n",
    "        trained_models[model_name] = {\n",
    "            'model': best_model,\n",
    "            'config': best_config\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBest {model_name} - config: {best_config}, val_loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88e910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM\n",
      "LSTM target shape: (120,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 19:19:25.206703: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-11-04 19:19:25.206747: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-11-04 19:19:25.206754: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-11-04 19:19:25.206785: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-04 19:19:25.206795: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-11-04 19:19:25.568291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    For config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}: val_loss = 0.8060\n",
      "    For config: {'hidden': 32, 'epochs': 100, 'dropout': 0.0}: val_loss = 1.4612\n",
      "    For config: {'hidden': 64, 'epochs': 100, 'dropout': 0.2}: val_loss = 1.1265\n",
      "\n",
      "Best LSTM - config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}, val_loss: 0.8060\n",
      "\n",
      "Training Bi-LSTM\n",
      "Bi-LSTM target shape: (120,)\n",
      "    For config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}: val_loss = 0.6709\n",
      "    For config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}: val_loss = 0.7458\n",
      "    For config: {'hidden': 32, 'epochs': 100, 'dropout': 0.2}: val_loss = 1.1401\n",
      "\n",
      "Best Bi-LSTM - config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}, val_loss: 0.6709\n",
      "\n",
      "Training ED-LSTM\n",
      "ED-LSTM target shape: (120, 1, 1)\n",
      "    For config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}: val_loss = 0.5623\n",
      "    For config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}: val_loss = 0.7009\n",
      "    For config: {'hidden': 32, 'epochs': 100, 'dropout': 0.2}: val_loss = 0.8419\n",
      "\n",
      "Best ED-LSTM - config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}, val_loss: 0.5623\n",
      "\n",
      "Training CNN\n",
      "CNN target shape: (120,)\n",
      "    For config: {'filters': 16, 'epochs': 100}: val_loss = 1.0551\n",
      "    For config: {'filters': 32, 'epochs': 100}: val_loss = 1.2817\n",
      "    For config: {'filters': 64, 'epochs': 100}: val_loss = 1.0956\n",
      "\n",
      "Best CNN - config: {'filters': 16, 'epochs': 100}, val_loss: 1.0551\n"
     ]
    }
   ],
   "source": [
    "trained_models = train_model_and_tuning(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "155ddf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def eval_on_test(train_df, test_df, target, features, scaler, trained_models, selected_countries):\n",
    "    all_results = {}\n",
    "\n",
    "    for country in selected_countries:\n",
    "        print(f\"{country.upper()}\")\n",
    "\n",
    "        train_country = train_df[train_df['country'] == country].sort_values('year').reset_index(drop=True)\n",
    "        test_country = test_df[test_df['country'] == country].sort_values('year').reset_index(drop=True)\n",
    "\n",
    "        combined_data = pd.concat([train_country, test_country], ignore_index=True)\n",
    "\n",
    "        # Calculating pct_change for combined data\n",
    "        combined_pct, _ = calculate_pct_change(combined_data, features, MAX_LAGS)\n",
    "\n",
    "        # Stack pct_change values for all features\n",
    "        feature_pct_values = []\n",
    "        for feat in features:\n",
    "            pct_col = f\"{feat}_pct_change\"\n",
    "            if pct_col in combined_pct.columns:\n",
    "                pct_values = combined_pct[pct_col].fillna(0).values\n",
    "                feature_pct_values.append(pct_values)\n",
    "\n",
    "        stacked_combined_pct = np.column_stack(feature_pct_values)\n",
    "        \n",
    "        # Fit the StandardScaler used before\n",
    "        combined_scaled = scaler.transform(stacked_combined_pct)\n",
    "\n",
    "        # Original values for calculating errors\n",
    "        original_values = combined_data[target].values\n",
    "\n",
    "        X_all, y_all = create_sequences(combined_scaled, N_STEPS_IN, len(features))\n",
    "\n",
    "        X_test = X_all[-TEST_SIZE:]\n",
    "        test_years = test_country['year'].values\n",
    "\n",
    "        print(f\"Test sequence shape: {X_test.shape}\")\n",
    "\n",
    "        country_results = {}\n",
    "\n",
    "        for model_name, model_info in trained_models.items():\n",
    "            model = model_info['model']\n",
    "            config = model_info['config']\n",
    "\n",
    "            # The model predicts scaled values due to the scaled train data\n",
    "            preds_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "            # Flatten for ED-LSTM 3D output (n_samples, n_steps_out, features)\n",
    "            if len(preds_scaled.shape) == 3:\n",
    "                preds_scaled = preds_scaled[:, 0, 0]\n",
    "            else:\n",
    "                # Others flatten with ravel\n",
    "                preds_scaled = preds_scaled.ravel()\n",
    "\n",
    "            # Inverse transform co2 (first feature)\n",
    "            tmp_array = np.zeros((len(preds_scaled), len(features)))\n",
    "            tmp_array[:, 0] = preds_scaled\n",
    "            preds_pct_inverse = scaler.inverse_transform(tmp_array)\n",
    "            preds_pct_change = preds_pct_inverse[:, 0]\n",
    "\n",
    "            # Denormalise pct change to forecasted actual CO2 values\n",
    "            forecast = []\n",
    "            test_start_idx = len(original_values) - TEST_SIZE\n",
    "\n",
    "            for i in range(TEST_SIZE):\n",
    "                prev_value = original_values[test_start_idx + i - 1]\n",
    "                predicted_value = prev_value * (1 + preds_pct_change[i] / 100)\n",
    "                forecast.append(predicted_value)\n",
    "\n",
    "            forecast = np.array(forecast)\n",
    "            actual_test = original_values[-TEST_SIZE:]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            individual_errors = np.abs(actual_test - forecast)\n",
    "            rmse_score = np.sqrt(mean_squared_error(actual_test, forecast))\n",
    "            mase_score = mase(actual_test, forecast)\n",
    "\n",
    "            country_results[model_name] = {\n",
    "                'forecast': forecast,\n",
    "                'actual': actual_test,\n",
    "                'individual_errors': individual_errors,\n",
    "                'test_years': test_years,\n",
    "                'RMSE': rmse_score,\n",
    "                'MASE': mase_score,\n",
    "                'best_config': config\n",
    "            }\n",
    "\n",
    "            print(f\"\\n  {model_name}\")\n",
    "            print(f\"    RMSE: {rmse_score:.4f}\")\n",
    "            print(f\"    MASE: {mase_score:.4f}\")\n",
    "            print(f\"    Config: {config}\")\n",
    "        \n",
    "        all_results[country] = country_results\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fac38948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITED STATES\n",
      "Test sequence shape: (9, 5, 8)\n",
      "\n",
      "  LSTM\n",
      "    RMSE: 376.5223\n",
      "    MASE: 1.6369\n",
      "    Config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  Bi-LSTM\n",
      "    RMSE: 413.9216\n",
      "    MASE: 1.9449\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  ED-LSTM\n",
      "    RMSE: 340.9380\n",
      "    MASE: 1.4963\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  CNN\n",
      "    RMSE: 363.2374\n",
      "    MASE: 1.5657\n",
      "    Config: {'filters': 16, 'epochs': 100}\n",
      "CHINA\n",
      "Test sequence shape: (9, 5, 8)\n",
      "\n",
      "  LSTM\n",
      "    RMSE: 582.5533\n",
      "    MASE: 1.9768\n",
      "    Config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  Bi-LSTM\n",
      "    RMSE: 636.3770\n",
      "    MASE: 2.2832\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  ED-LSTM\n",
      "    RMSE: 656.0639\n",
      "    MASE: 2.3311\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  CNN\n",
      "    RMSE: 658.6271\n",
      "    MASE: 2.3431\n",
      "    Config: {'filters': 16, 'epochs': 100}\n",
      "INDIA\n",
      "Test sequence shape: (9, 5, 8)\n",
      "\n",
      "  LSTM\n",
      "    RMSE: 145.8430\n",
      "    MASE: 0.7527\n",
      "    Config: {'hidden': 16, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  Bi-LSTM\n",
      "    RMSE: 164.8177\n",
      "    MASE: 0.8586\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  ED-LSTM\n",
      "    RMSE: 152.8127\n",
      "    MASE: 0.7879\n",
      "    Config: {'hidden': 8, 'epochs': 100, 'dropout': 0.0}\n",
      "\n",
      "  CNN\n",
      "    RMSE: 144.9349\n",
      "    MASE: 0.7981\n",
      "    Config: {'filters': 16, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "dl_results = eval_on_test(train_3_df, test_3_df, TARGET_VARIABLES, MULTIVARIATE_FEATURES,\n",
    "                          scaler, trained_models, SELECTED_COUNTRIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2b5db",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed2ea664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country     China     India  United States\n",
      "Model                                     \n",
      "LSTM     582.5533  145.8430       376.5223\n",
      "Bi-LSTM  636.3770  164.8177       413.9216\n",
      "ED-LSTM  656.0639  152.8127       340.9380\n",
      "CNN      658.6271  144.9349       363.2374\n"
     ]
    }
   ],
   "source": [
    "summary_data = []\n",
    "\n",
    "for country in SELECTED_COUNTRIES:\n",
    "    for model_name, result in dl_results[country].items():\n",
    "        summary_data.append({\n",
    "            'Country': country,\n",
    "            'Model': model_name,\n",
    "            'RMSE': result['RMSE'],\n",
    "            'MASE': result['MASE']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# RMSE pivot table\n",
    "rmse_pivot = summary_df.pivot(index='Model', columns='Country', values='RMSE')\n",
    "rmse_pivot = rmse_pivot.round(4)\n",
    "\n",
    "# Ordering models\n",
    "model_order = ['LSTM', 'Bi-LSTM', 'ED-LSTM', 'CNN']\n",
    "rmse_pivot = rmse_pivot.reindex([m for m in model_order if m in rmse_pivot.index])\n",
    "\n",
    "print(rmse_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22287db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country   China   India  United States\n",
      "Model                                 \n",
      "LSTM     1.9768  0.7527         1.6369\n",
      "Bi-LSTM  2.2832  0.8586         1.9449\n",
      "ED-LSTM  2.3311  0.7879         1.4963\n",
      "CNN      2.3431  0.7981         1.5657\n"
     ]
    }
   ],
   "source": [
    "# MASE pivot table\n",
    "mase_pivot = summary_df.pivot(index='Model', columns='Country', values='MASE')\n",
    "mase_pivot = mase_pivot.round(4)\n",
    "\n",
    "# Ordering models\n",
    "model_order = ['LSTM', 'Bi-LSTM', 'ED-LSTM', 'CNN']\n",
    "mase_pivot = mase_pivot.reindex([m for m in model_order if m in mase_pivot.index])\n",
    "\n",
    "print(mase_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table_filepath = os.path.join(save_dir, 'Multi_DL_summary.md')\n",
    "with open(pivot_table_filepath, 'w') as f:\n",
    "    f.write(\"# Summary of Multivariate DL models\\n\\n\")\n",
    "    f.write(\"RMSE of each DL model trained on all countries combined dataset with selected features for 3 countries\\n\\n\")\n",
    "    f.write(rmse_pivot.to_markdown())\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "    f.write(\"---\\n\\n\")\n",
    "\n",
    "    f.write(\"MASE of each DL model trained on all countries combined dataset with selected features for 3 countries\\n\\n\")\n",
    "    f.write(mase_pivot.to_markdown())\n",
    "    f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc00240",
   "metadata": {},
   "source": [
    "### Visualisation of forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66dab15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d38283c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
