{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ae2f74",
   "metadata": {},
   "source": [
    "# Autoregression Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c556638",
   "metadata": {},
   "source": [
    "### Imports and Load data from 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4bdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "import statsmodels.stats.diagnostic\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Memory control\n",
    "physical_devices = tf.config.list_logical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Loading data\n",
    "def load_dfs(save_dir='data_export'):\n",
    "\n",
    "    g20_path = os.path.join(save_dir, 'g20_df_1965.pkl')\n",
    "    lag_path = os.path.join(save_dir, 'lag_df_1965.pkl')\n",
    "\n",
    "    try:\n",
    "        g20_df = pd.read_pickle(g20_path)\n",
    "        print(f\"Successfully loaded g20 dataframe pickle file\")\n",
    "        lag_df = pd.read_pickle(lag_path)\n",
    "        print(f\"Successfully loaded time lagged g20 dataframe pickle file\")\n",
    "\n",
    "        return g20_df, lag_df\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error in loading pickle files: {e}\")\n",
    "\n",
    "        try:\n",
    "            g20_csv = g20_path.replace('.pkl', '.csv')\n",
    "            lag_csv = lag_path.replace('.pkl', '.csv')\n",
    "\n",
    "            g20_df = pd.read_csv(g20_csv)\n",
    "            print(f\"Successfully loaded g20 dataframe csv file\")\n",
    "            lag_df = pd.read_csv(lag_csv)\n",
    "            print(f\"Successfully loaded time lagged g20 dataframe csv file\")\n",
    "\n",
    "            return g20_df, lag_df\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error in loading csv files: {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a6565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded g20 dataframe pickle file\n",
      "Successfully loaded time lagged g20 dataframe pickle file\n"
     ]
    }
   ],
   "source": [
    "g20_df, lag_df = load_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c0b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "target_variables = ['co2', 'gdp', 'primary_energy_consumption']\n",
    "n_steps_in = 5\n",
    "n_steps_out = 3\n",
    "g20_countries = [\n",
    "    'United States', 'China', 'Japan', 'Germany', \n",
    "    'United Kingdom', 'France', 'Italy', 'Canada',\n",
    "    'Brazil', 'Russia', 'India', 'Australia', \n",
    "    'Mexico', 'Indonesia', 'Turkey', 'Saudi Arabia',\n",
    "    'South Africa', 'Argentina', 'South Korea'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f2571",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f694384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimised memory\n",
    "def clear_memory():\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        tf.config.experimental.reset_memory_stats('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd2293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for RMSE\n",
    "def rmse(pred, actual):\n",
    "    return np.sqrt(((pred - actual)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3a7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for MASE\n",
    "def mase(actual, pred, period=1):\n",
    "    \"\"\"\n",
    "    MASE = MAE / naive_forecast_MAE(average magnitude of errors)\n",
    "    \"\"\"\n",
    "\n",
    "    # MAE of prediction\n",
    "    mae_forecast = mean_absolute_error(actual, pred)\n",
    "\n",
    "    # MAE of naive forecast with previous period\n",
    "    naive_forecast = actual[:-period] if period > 0 else actual[:-1]\n",
    "    actual_for_naive = actual[period:] if period > 0 else actual[1:]\n",
    "\n",
    "    if len(naive_forecast) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    mae_naive = mean_absolute_error(actual_for_naive, naive_forecast)\n",
    "\n",
    "    if mae_naive == 0:\n",
    "        return 0 if mae_forecast == 0 else np.inf\n",
    "    \n",
    "    return mae_forecast / mae_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e629095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking stationary \n",
    "def check_stationary(data, alpha=0.05):\n",
    "\n",
    "    result = adfuller(data)\n",
    "    p_value = result[1]\n",
    "    #print(f\"ADF test p-value: {p_value:.4f}, Critical value: {alpha}\")\n",
    "    return p_value < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aa68355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration using Engle-Granger test\n",
    "def check_coint(y, X, alpha=0.05):\n",
    "        \n",
    "    _, p_value, _ = coint(y, X)\n",
    "    #print(f\"Coint test p-value: {p_value:.4f}\")\n",
    "\n",
    "    return p_value < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7ce450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse differencing\n",
    "def inv_diff(forecast_diff, last_value):\n",
    "\n",
    "    if last_value is None:\n",
    "        return forecast_diff\n",
    "    \n",
    "    # Cumulative sum\n",
    "    cum_values = [last_value]\n",
    "    for diff_val in forecast_diff:\n",
    "        cum_values.append(cum_values[-1] + diff_val)\n",
    "\n",
    "    return np.array(cum_values[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ec9f3",
   "metadata": {},
   "source": [
    "### Data Preperation and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62c6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_info(target_var):\n",
    "\n",
    "    scale_info = {\n",
    "        'co2': {\n",
    "            'unit': 'Million tonnes CO2'\n",
    "        },\n",
    "        'gdp': {\n",
    "            'unit': 'USD'\n",
    "        },\n",
    "        'primary_energy_consumption': {\n",
    "            'unit': 'Thousand TWh'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return scale_info.get(target_var, {'Unit': 'Original'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3851d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAR Checkpoints:\n",
    "    Check stationary (Same as ARIMA)\n",
    "    Check long-term stable relationship (statioanry) for two or more.\n",
    "\"\"\"\n",
    "\n",
    "# Preparing data for VAR\n",
    "def prepare_var_data(X_train, target_col=0):\n",
    "\n",
    "    orig_data = X_train.copy()\n",
    "    processed_series = []\n",
    "\n",
    "    # Test stationarity and diff & inverse diff\n",
    "    diff_applied = []\n",
    "    last_values = []\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        series = orig_data[:, i].copy()\n",
    "\n",
    "        if check_stationary(series):\n",
    "            #print(f\"Variable {i} is stationary\")\n",
    "            processed_series.append(series)\n",
    "            diff_applied.append(0)\n",
    "            last_values.append(None)\n",
    "        else:\n",
    "            last_values.append(series[-1])\n",
    "            diff_series = np.diff(series)\n",
    "            processed_series.append(diff_series)\n",
    "            diff_applied.append(1)\n",
    "    \n",
    "    min_length = min(len(series) for series in processed_series)\n",
    "    trim_series = [series[-min_length:] for series in processed_series]\n",
    "    trim_data = np.column_stack(trim_series)\n",
    "\n",
    "    # Test cointegration with target\n",
    "    target_series = trim_data[:, target_col]\n",
    "    coint_vars = [target_col]\n",
    "\n",
    "    for i in range(trim_data.shape[1]):\n",
    "        if i != target_col:\n",
    "            if check_coint(target_series, trim_data[:, i]):\n",
    "                #print(f\"Variable {i} is cointegrated with target\")\n",
    "                coint_vars.append(i)\n",
    "\n",
    "    # Select coint vars\n",
    "    selected_data = trim_data[:, coint_vars]\n",
    "    selected_diff = [diff_applied[i] for i in coint_vars]\n",
    "    selected_last = [last_values[i] for i in coint_vars]\n",
    "\n",
    "    print(f\"Selected vars: {coint_vars}\")\n",
    "\n",
    "    del orig_data\n",
    "    gc.collect()\n",
    "\n",
    "    return selected_data, coint_vars, selected_diff, selected_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736cb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Autoregression data\n",
    "def prepare_ar_data(country, target_var, lag_df, n_steps_in=5):\n",
    "    \"\"\"\n",
    "    n_steps_in=5 : current value + 4 lags\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nPreparing Autoregressive datafor {country}: {target_var}\")\n",
    "\n",
    "    # Country data from lag_df\n",
    "    country_data = lag_df[lag_df['country'] == country].copy()\n",
    "    country_data = country_data.sort_values('year')\n",
    "\n",
    "    # Columns: target + 4 lags\n",
    "    feature_cols = [target_var]\n",
    "    for i in range(1, n_steps_in):\n",
    "        lag_col = f\"{target_var}_lag{i}\"\n",
    "        if lag_col in country_data.columns:\n",
    "            feature_cols.append(lag_col)\n",
    "        else:\n",
    "            print(f\"No {lag_col} for {country}\")\n",
    "        \n",
    "    # Extract data and remove na\n",
    "    data_subset = country_data[['year'] + feature_cols].copy()\n",
    "    data_subset = data_subset.dropna()\n",
    "\n",
    "    years = data_subset['year'].values\n",
    "\n",
    "    # Unit information\n",
    "    scale_info = get_target_info(target_var)\n",
    "\n",
    "    features = []\n",
    "    for col in feature_cols:\n",
    "        features.append(data_subset[col].values)\n",
    "\n",
    "    ar_data = np.column_stack(features)\n",
    "    target_values = features[0]\n",
    "\n",
    "    print(f\"Features used: {feature_cols}\")\n",
    "    print(f\"Data shape: {ar_data.shape}\")\n",
    "    print(f\"Unit: {scale_info['unit']}\")\n",
    "    print(f\"Target range: {np.min(target_values):.2f} - {np.max(target_values):.2f}\")\n",
    "\n",
    "    return ar_data, years, target_values, feature_cols, scale_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23e938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences to make it 3 steps out\n",
    "def create_seq_for_pred(ar_data, n_steps_out=3):\n",
    "    \"\"\"\n",
    "    [t-4, t-3, t-2, t-1, t] -> [t+1, t+2, t+3]\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(ar_data) - n_steps_out):\n",
    "        # [t, t-1, t-2, t-3, t-4]: Shape (5,)\n",
    "        X_seq = ar_data[i]\n",
    "        \n",
    "        # [t+1, t+2, t+3]\n",
    "        y_seq = []\n",
    "        for j in range(1, n_steps_out + 1):\n",
    "\n",
    "            if i + j < len(ar_data):\n",
    "                # First column is the target\n",
    "                y_seq.append(ar_data[i + j, 0])\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if len(y_seq) == n_steps_out:\n",
    "            X.append(X_seq)\n",
    "            y.append(y_seq)\n",
    "\n",
    "    # Shape (n_samples, 5)\n",
    "    X = np.array(X)\n",
    "    # Shape (n_samples, 3)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc731f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for DL to handle 3D inputs and scaling\n",
    "def prepare_dl_data(X, y):\n",
    "    \n",
    "    # Shape (n_samples, 5, 1)\n",
    "\n",
    "    # Scaler (2D)\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # (n_samples, 5)\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    # (n_samples, 3)\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "\n",
    "    X_dl_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "    return X_dl_scaled, y_scaled, X_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efadd9",
   "metadata": {},
   "source": [
    "### Constructing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29bc5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning ARIMA - AIC criterion\n",
    "def tune_arima_params(data, max_p=4, max_d=2, max_q=3):\n",
    "    \"\"\"\n",
    "    p (autoregressive order): num of lag obs to include\n",
    "    d (differencing degree): num of time data different to make it stationary\n",
    "    q (moving avg order): size of moving avg window\n",
    "    Lower AIC -> better model fit\n",
    "    \"\"\"\n",
    "\n",
    "    best_aic = np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    # Testing stationarity\n",
    "    result = adfuller(data)\n",
    "    max_d = 1 if result[1] <= 0.05 else 2\n",
    "\n",
    "    for p in range(max_p + 1):\n",
    "        for d in range(max_d + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    model = ARIMA(data, order=(p, d, q))\n",
    "                    fitted_model = model.fit()\n",
    "                    aic = fitted_model.aic\n",
    "\n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_params = (p, d, q)\n",
    "                        best_model = fitted_model\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Best ARIMA params: {best_params}, AIC: {best_aic:.2f}\")\n",
    "\n",
    "    return best_model, best_params, best_aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75073fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning VAR \n",
    "# -> it might cause overfitting since there are too less tranining samples compared to the number of features\n",
    "def tune_var_params(data, max_lags=4):\n",
    "    \"\"\"\n",
    "    Each variable is modeled as linear of past values of itself (other variables but not for this data)\n",
    "    lag_order -> num of lag periods to include\n",
    "    using AIC to select optimal lag order.\n",
    "    \"\"\"\n",
    "\n",
    "    var_df = pd.DataFrame(data, columns=[f'var{i}' for i in range(data.shape[1])])\n",
    "    model = VAR(var_df)\n",
    "\n",
    "    obs_lag = len(var_df) // (data.shape[1] * 4)\n",
    "    eff_max_lags = min(max_lags, obs_lag)\n",
    "    eff_max_lags = max(1, eff_max_lags)\n",
    "\n",
    "    lag_result = model.select_order(maxlags=eff_max_lags)\n",
    "    best_lag = lag_result.selected_orders['aic']\n",
    "    print(f\"Best lag order: {best_lag}\")\n",
    "\n",
    "    fitted_model = model.fit(best_lag)\n",
    "\n",
    "    del var_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return fitted_model, best_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e00b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive vanilla LSTM\n",
    "def build_ar_lstm(input_shape, hidden=16):\n",
    "    model = Sequential([\n",
    "        LSTM(hidden, activation='relu', input_shape=input_shape,\n",
    "             kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),\n",
    "        Dense(n_steps_out)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2baa1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive Bidirectional LSTM\n",
    "def build_ar_bilstm(input_shape, hidden=8):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(hidden, activation='relu', kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),\n",
    "                       input_shape=input_shape),\n",
    "        Dense(n_steps_out)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4453ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive Encoder-Decoder LSTM\n",
    "def build_ar_edlstm(input_shape, hidden=8):\n",
    "    model = Sequential([\n",
    "        LSTM(hidden, activation='relu', input_shape=input_shape,\n",
    "             kernel_regularizer=l2(0.001)),\n",
    "        RepeatVector(n_steps_out),\n",
    "        LSTM(hidden, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=l2(0.001)),\n",
    "        TimeDistributed(Dense(1))\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9af1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive CNN\n",
    "def build_ar_cnn(input_shape, filters=32, hidden=8):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=3, activation='relu', input_shape=input_shape, padding='same',\n",
    "               kernel_regularizer=l2(0.001)),\n",
    "        Flatten(),\n",
    "        Dense(hidden, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(n_steps_out)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e65be2",
   "metadata": {},
   "source": [
    "### Model training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be5f61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ARIMA\n",
    "def train_arima_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    try:\n",
    "        # Using first column of the target (1D)\n",
    "        target_train = X_train[:, 0]\n",
    "\n",
    "        model, params, aic = tune_arima_params(target_train)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # Forecasting len(y_test) steps\n",
    "        for i in range(len(y_test)):\n",
    "            try:\n",
    "                # Forecasting 1 step\n",
    "                forecast_result = model.forecast(steps=1)\n",
    "                # Check if its not a sequence (array, list, pandas Series) \n",
    "                pred_value = forecast_result[0] if hasattr(forecast_result, '__len__') else forecast_result\n",
    "                preds.append(pred_value)\n",
    "\n",
    "                # Update the model with actual for the next pred\n",
    "                if i < len(y_test) - 1:\n",
    "                    actual_next = y_test[i, 0]\n",
    "                    new_data = np.append(target_train, actual_next)\n",
    "                    model = ARIMA(new_data, order=params).fit()\n",
    "                    target_train = new_data\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ARIMA pred step {i} failed {e}\")\n",
    "                preds.append(preds[-1] if preds else 0)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        actual_test = y_test[:, 0]\n",
    "\n",
    "        rmse_score = rmse(preds, actual_test)\n",
    "        mase_score = mase(actual_test, preds)\n",
    "\n",
    "        return preds, rmse_score, mase_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA training failed {e}\")\n",
    "\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49bd9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training VAR\n",
    "def train_var_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    try:\n",
    "        var_data, selected_vars, diff_orders, last_values = prepare_var_data(X_train)\n",
    "\n",
    "        fitted_model, best_lag = tune_var_params(var_data)\n",
    "\n",
    "        # Forecast input\n",
    "        if len(var_data) >= best_lag:\n",
    "            forecast_input = var_data[-best_lag:]\n",
    "        else:\n",
    "            forecast_input = var_data\n",
    "\n",
    "        # Forecast\n",
    "        n_steps = len(y_test)\n",
    "        forecast_result = fitted_model.forecast(forecast_input, steps=n_steps)\n",
    "\n",
    "        # Target preds\n",
    "        if len(forecast_result.shape) == 1:\n",
    "            preds_diff = forecast_result\n",
    "        else:\n",
    "            preds_diff = forecast_result[:, 0]\n",
    "\n",
    "        # Inverse differencing if needed\n",
    "        target_diff_order = diff_orders[0]\n",
    "        target_last_value = last_values[0]\n",
    "\n",
    "        if target_diff_order > 0:\n",
    "            preds = inv_diff(preds_diff, target_last_value)\n",
    "            #print(f\"Final predictions range: {preds.min():.2f} - {preds.max():.2f}\")\n",
    "        else:\n",
    "            preds = preds_diff\n",
    "\n",
    "        # Metrics\n",
    "        actual = y_test[:, 0]\n",
    "        min_length = min(len(preds), len(actual))\n",
    "        preds = preds[:min_length]\n",
    "        actual = actual[:min_length]\n",
    "\n",
    "        #print(f\"Actual value range: {actual.min():.2f} - {actual.max():.2f}\")\n",
    "\n",
    "        rmse_score = rmse(preds, actual)\n",
    "        mase_score = mase(actual, preds)\n",
    "\n",
    "        print(f\"VAR - RMSE: {rmse_score:.4f}, MASE: {mase_score:.4f}\")\n",
    "\n",
    "        return preds, rmse_score, mase_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"VAR training failed {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eecbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DL models\n",
    "def train_dl_model(model_build, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "\n",
    "    try:\n",
    "        # Prep data for dl\n",
    "        X_train_dl, y_train_scaled, _, y_scaler = prepare_dl_data(X_train, y_train)\n",
    "        X_val_dl, y_val_scaled, _, _ = prepare_dl_data(X_val, y_val)\n",
    "        X_test_dl, _, _, _ = prepare_dl_data(X_test, y_test)\n",
    "\n",
    "        # Build model\n",
    "        # (Steps, n_features)\n",
    "        input_shape = (X_train_dl.shape[1], X_train_dl.shape[2])\n",
    "        model = model_build(input_shape)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', patience=20, restore_best_weights=True, min_delta=0.01\n",
    "            )   # patience=15 since it a small dataset for each feature for each country\n",
    "                # min_delta=0.01 for less striction\n",
    "\n",
    "        # Calculate batch_size\n",
    "        batch_size = min(8, len(X_train_dl))\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_dl, y_train_scaled,\n",
    "            validation_data=(X_val_dl, y_val_scaled),\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        preds_scaled = model.predict(X_test_dl, verbose=0)\n",
    "\n",
    "        # Handling ED-LSTM 3D output (batch, steps, 1)\n",
    "        if len(preds_scaled.shape) == 3:\n",
    "            preds_scaled = preds_scaled.reshape(preds_scaled.shape[0], preds_scaled.shape[1])\n",
    "\n",
    "        preds = y_scaler.inverse_transform(preds_scaled)\n",
    "\n",
    "        # First step preds\n",
    "        pred_first = preds[:, 0]\n",
    "        actual_first = y_test[:, 0]\n",
    "\n",
    "        # Metrics\n",
    "        rmse_score = rmse(pred_first, actual_first)\n",
    "        mase_score = mase(actual_first, pred_first)\n",
    "\n",
    "        # Memory\n",
    "        del model, history, X_train_dl, y_train_scaled, X_val_dl, y_val_scaled, X_test_dl\n",
    "        clear_memory()\n",
    "\n",
    "        return pred_first, rmse_score, mase_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name} training failed {e}\")\n",
    "        clear_memory()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137516cf",
   "metadata": {},
   "source": [
    "### Testing Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c0075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_data(country, target_var, lag_df):\n",
    "\n",
    "    ar_data, years, target_values, feature_cols, scale_info = prepare_ar_data(\n",
    "        country, target_var, lag_df, n_steps_in\n",
    "    )\n",
    "\n",
    "    print(f\"Data Range: {np.min(target_values):.2f} - {np.max(target_values):.2f}\")\n",
    "    print(f\"Mean: {np.mean(target_values):.2f}\")\n",
    "    print(f\"Std: {np.std(target_values):.2f}\")\n",
    "\n",
    "    changes = np.diff(target_values)\n",
    "    print(f\"Avg yearly change: {np.mean(changes):.1f}\")\n",
    "    print(f\"Max yearly change: {np.max(np.abs(changes)):.1f}\")\n",
    "    \n",
    "    return {\n",
    "        'range': np.max(target_values) - np.min(target_values),\n",
    "        'mean': np.mean(target_values),\n",
    "        'growth_factor': target_values[-1]/target_values[0] if target_values[0] != 0 else np.inf,\n",
    "        'max_change': np.max(np.abs(changes)),\n",
    "        'data_points': len(target_values),\n",
    "        'years_range': (years[0], years[-1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f843968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test country\n",
    "def test_country_target(country, target_var, lag_df):\n",
    "\n",
    "    print(f\"Testing {country}: {target_var}\")\n",
    "\n",
    "    data_stats = analyse_data(country, target_var, lag_df)\n",
    "\n",
    "    # Prepare data\n",
    "    ar_data, years, target_values, feature_cols, scale_info = prepare_ar_data(\n",
    "        country, target_var, lag_df, n_steps_in=5\n",
    "    )\n",
    "\n",
    "    # Seq for pred\n",
    "    X, y = create_seq_for_pred(ar_data, n_steps_out)\n",
    "\n",
    "    \"\"\"\n",
    "    feature_df = pd.DataFrame(ar_data, columns=feature_cols)\n",
    "\n",
    "    try:    \n",
    "        train, val, test, X_train, y_train, X_val, y_val, X_test, y_test = shuffled_test_train_val(\n",
    "                n_steps_in, n_steps_out, feature_df, test_size=0.2, val_size=0.1\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with shuffled_test_train_val {e}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Time series seq split\n",
    "    train_size = int(0.7 * len(X))\n",
    "    val_size = int(0.15 * len(X))\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    X_val = X[train_size:train_size + val_size]\n",
    "    y_val = y[train_size:train_size + val_size]\n",
    "    X_test = X[train_size + val_size:]\n",
    "    y_test = y[train_size + val_size:]\n",
    "\n",
    "    # Test years for plots\n",
    "    test_start_idx = train_size + val_size\n",
    "    test_years = years[test_start_idx:test_start_idx + len(X_test)]\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Testing models\n",
    "    model_configs = [\n",
    "        ('ARIMA', lambda: train_arima_model(X_train, y_train, X_test, y_test)),\n",
    "        ('VAR', lambda: train_var_model(X_train, y_train, X_test, y_test)),\n",
    "        ('LSTM', lambda: train_dl_model(build_ar_lstm, X_train, y_train, X_val, y_val, X_test, y_test, 'LSTM')),\n",
    "        ('Bi-LSTM', lambda: train_dl_model(build_ar_bilstm, X_train, y_train, X_val, y_val, X_test, y_test, 'Bi-LSTM')),\n",
    "        ('ED-LSTM', lambda: train_dl_model(build_ar_edlstm, X_train, y_train, X_val, y_val, X_test, y_test, 'ED-LSTM')),\n",
    "        ('CNN', lambda: train_dl_model(build_ar_cnn, X_train, y_train, X_val, y_val, X_test, y_test, 'CNN'))\n",
    "    ]\n",
    "\n",
    "    for model_name, train_func in model_configs:\n",
    "        print(f\"\\nTraining {model_name}\")\n",
    "\n",
    "        preds, rmse_score, mase_score = train_func()\n",
    "\n",
    "        if preds is not None:\n",
    "            print(f\"{model_name} - RMSE:{rmse_score:.4f}, MASE:{mase_score:.4f}\")\n",
    "\n",
    "            results[model_name] = {\n",
    "                'predictions': preds,\n",
    "                'rmse': rmse_score,\n",
    "                'mase': mase_score,\n",
    "                'actual': y_test[:, 0],\n",
    "                'test_years': test_years,\n",
    "                'scale_info': scale_info\n",
    "            }\n",
    "        else:\n",
    "            print(f\"{model_name} failed\")\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ff32fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing United States: co2\n",
      "\n",
      "Preparing Autoregressive datafor United States: co2\n",
      "Features used: ['co2', 'co2_lag1', 'co2_lag2', 'co2_lag3', 'co2_lag4']\n",
      "Data shape: (59, 5)\n",
      "Unit: Million tonnes CO2\n",
      "Target range: 3399.55 - 6132.18\n",
      "Data Range: 3399.55 - 6132.18\n",
      "Mean: 5091.87\n",
      "Std: 650.04\n",
      "Avg yearly change: 26.1\n",
      "Max yearly change: 547.5\n",
      "\n",
      "Preparing Autoregressive datafor United States: co2\n",
      "Features used: ['co2', 'co2_lag1', 'co2_lag2', 'co2_lag3', 'co2_lag4']\n",
      "Data shape: (59, 5)\n",
      "Unit: Million tonnes CO2\n",
      "Target range: 3399.55 - 6132.18\n",
      "Train: 39, Val: 8, Test: 9\n",
      "\n",
      "Training ARIMA\n",
      "Best ARIMA params: (0, 2, 2), AIC: 472.78\n",
      "ARIMA - RMSE:350.5722, MASE:1.5457\n",
      "\n",
      "Training VAR\n",
      "Selected vars: [0, 1, 2, 3, 4]\n",
      "Best lag order: 1\n",
      "VAR - RMSE: 1114.4355, MASE: 5.5370\n",
      "VAR - RMSE:1114.4355, MASE:5.5370\n",
      "\n",
      "Training LSTM\n",
      "LSTM - RMSE:374.7862, MASE:1.8542\n",
      "\n",
      "Training Bi-LSTM\n",
      "Bi-LSTM - RMSE:259.6192, MASE:1.2445\n",
      "\n",
      "Training ED-LSTM\n",
      "ED-LSTM - RMSE:374.5673, MASE:1.8379\n",
      "\n",
      "Training CNN\n",
      "CNN - RMSE:431.7267, MASE:2.0434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ARIMA': {'predictions': array([6069.94348918, 5196.38483912, 5593.37152913, 5316.68598825,\n",
       "         5176.89795913, 5152.32044643, 5402.19290002, 5222.78245029,\n",
       "         4515.67496635]),\n",
       "  'rmse': np.float64(350.57215344821986),\n",
       "  'mase': np.float64(1.5456812675940859),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}},\n",
       " 'VAR': {'predictions': array([6036.74731982, 6087.73069256, 6165.94138558, 6231.39523472,\n",
       "         6292.80440469, 6361.6847755 , 6428.37640855, 6491.35308182,\n",
       "         6556.03977524]),\n",
       "  'rmse': np.float64(1114.4355269510968),\n",
       "  'mase': np.float64(5.536991896706228),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}},\n",
       " 'LSTM': {'predictions': array([4972.488 , 4963.6826, 4959.0195, 4955.318 , 4949.37  , 4947.229 ,\n",
       "         4934.056 , 4926.8594, 4900.849 ], dtype=float32),\n",
       "  'rmse': np.float64(374.78616936845617),\n",
       "  'mase': np.float64(1.8541822401059267),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}},\n",
       " 'Bi-LSTM': {'predictions': array([5311.106 , 5133.2617, 5190.125 , 5120.2764, 4975.4053, 4941.149 ,\n",
       "         4987.1274, 4914.3438, 4947.8027], dtype=float32),\n",
       "  'rmse': np.float64(259.61920334326385),\n",
       "  'mase': np.float64(1.2444547552144825),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}},\n",
       " 'ED-LSTM': {'predictions': array([4959.657 , 4958.933 , 4959.2197, 4957.4946, 4952.7847, 4954.321 ,\n",
       "         4949.7114, 4941.8906, 4932.624 ], dtype=float32),\n",
       "  'rmse': np.float64(374.56729210806907),\n",
       "  'mase': np.float64(1.8378903183096142),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}},\n",
       " 'CNN': {'predictions': array([5072.2974, 4870.108 , 4941.783 , 4940.092 , 4819.9014, 4769.658 ,\n",
       "         4770.892 , 4723.9434, 4872.582 ], dtype=float32),\n",
       "  'rmse': np.float64(431.72672095636085),\n",
       "  'mase': np.float64(2.0434257163809653),\n",
       "  'actual': array([5480.157, 5528.681, 5376.473, 5252.932, 5212.162, 5377.797,\n",
       "         5262.145, 4714.628, 5032.213]),\n",
       "  'test_years': array([2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]),\n",
       "  'scale_info': {'unit': 'Million tonnes CO2'}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'United States'\n",
    "target_var = 'co2'\n",
    "test_country_target(country, target_var, lag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c83a5",
   "metadata": {},
   "source": [
    "### Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80525832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests():\n",
    "\n",
    "    all_results = defaultdict(lambda: defaultdict(dict))\n",
    "    summary_stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for target_var in target_variables:\n",
    "        print(f\"\\nTesting target: {target_var.upper()}\")\n",
    "\n",
    "        for country in g20_countries:\n",
    "            country_results = test_country_target(country, target_var, lag_df)\n",
    "\n",
    "            all_results[target_var][country] = country_results\n",
    "\n",
    "            for model_name, metrics in country_results.items():\n",
    "                summary_stats[target_var][model_name].append({\n",
    "                    'country': country,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mase': metrics['mase'],\n",
    "                    'scale_info': metrics['scale_info']\n",
    "                })\n",
    "                \n",
    "        clear_memory()\n",
    "\n",
    "    return all_results, summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97196024",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results, summary_stats = run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9755129",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "413e5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_results(summary_stats, save_dir='data_export'):\n",
    "    \n",
    "    print(\"\\nSummary Statistics\")\n",
    "    \n",
    "    # Create save directory\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Directory created\")\n",
    "\n",
    "    summary_df_list = []\n",
    "    \n",
    "    for target_var in target_variables:\n",
    "        scale_info = get_target_info(target_var)\n",
    "        print(f\"\\n{target_var.upper()} Results (Unit: {scale_info['unit']})\")\n",
    "        \n",
    "        for model_name in ['ARIMA', 'VAR', 'LSTM', 'Bi-LSTM', 'ED-LSTM', 'CNN']:\n",
    "            if model_name in summary_stats[target_var] and summary_stats[target_var][model_name]:\n",
    "\n",
    "                stats = summary_stats[target_var][model_name]\n",
    "                rmse_values = [s['rmse'] for s in stats]\n",
    "                mase_values = [s['mase'] for s in stats]\n",
    "                \n",
    "                summary_df_list.append({\n",
    "                    'Target': target_var,\n",
    "                    'Model': model_name,\n",
    "                    'Unit': scale_info['unit'],\n",
    "                    'Countries': len(stats),\n",
    "                    'Avg_RMSE': np.mean(rmse_values),\n",
    "                    'Min_RMSE': np.min(rmse_values),\n",
    "                    'Max_RMSE': np.max(rmse_values),\n",
    "                    'Avg_MASE': np.mean(mase_values),\n",
    "                    'Min_MASE': np.min(mase_values),\n",
    "                    'Max_MASE': np.max(mase_values)\n",
    "                })\n",
    "                \n",
    "                print(f\"{model_name:>10}: Countries={len(stats):>2}, \"\n",
    "                      f\"RMSE: {np.mean(rmse_values):>8.3f} [{np.min(rmse_values):>8.3f}-{np.max(rmse_values):>8.3f}], \"\n",
    "                      f\"MASE: {np.mean(mase_values):>6.2f} [{np.min(mase_values):>6.2f}-{np.max(mase_values):>6.2f}]\")\n",
    "            else:\n",
    "                print(f\"{model_name:>10}: No results\")\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_df_list)\n",
    "\n",
    "    # Save as csv\n",
    "    summary_csv_path = os.path.join(save_dir, 'ar_summay.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd1a09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "CO2 Results (Unit: Million tonnes CO2)\n",
      "     ARIMA: Countries=19, RMSE:  205.279 [  13.555-2243.321], MASE:   1.94 [  0.81-  4.87]\n",
      "       VAR: Countries=19, RMSE:  417.120 [  26.895-4018.751], MASE:   5.94 [  1.47- 16.48]\n",
      "      LSTM: Countries=19, RMSE:  742.490 [  78.146-8206.255], MASE:  12.62 [  1.69- 33.51]\n",
      "   Bi-LSTM: Countries=19, RMSE:  748.861 [  77.389-8217.842], MASE:  12.57 [  2.18- 33.66]\n",
      "   ED-LSTM: Countries=19, RMSE:  751.374 [  43.168-8321.348], MASE:  12.60 [  1.83- 34.00]\n",
      "       CNN: Countries=19, RMSE:  765.305 [  69.859-8285.032], MASE:  13.00 [  2.42- 33.94]\n",
      "\n",
      "GDP Results (Unit: Trillion USD)\n",
      "     ARIMA: Countries=19, RMSE:    0.111 [   0.025-   0.320], MASE:   3.42 [  1.21-  8.81]\n",
      "       VAR: Countries=19, RMSE:    0.242 [   0.038-   0.694], MASE:  11.36 [  1.87- 35.81]\n",
      "      LSTM: Countries=19, RMSE:    0.644 [   0.287-   1.749], MASE:  30.95 [ 17.15- 45.16]\n",
      "   Bi-LSTM: Countries=19, RMSE:    0.647 [   0.286-   1.758], MASE:  30.98 [ 16.00- 44.80]\n",
      "   ED-LSTM: Countries=19, RMSE:    0.639 [   0.291-   1.710], MASE:  30.88 [ 16.22- 44.89]\n",
      "       CNN: Countries=19, RMSE:    0.648 [   0.284-   1.743], MASE:  31.17 [ 17.08- 45.35]\n",
      "\n",
      "PRIMARY_ENERGY_CONSUMPTION Results (Unit: Thousand TWh)\n",
      "     ARIMA: Countries=19, RMSE:    0.856 [   0.057-  10.316], MASE:   2.03 [  1.07-  5.39]\n",
      "       VAR: Countries=18, RMSE:    1.006 [   0.083-   6.668], MASE:   5.08 [  1.43-  8.40]\n",
      "      LSTM: Countries=19, RMSE:    2.902 [   0.184-  30.011], MASE:  13.32 [  1.98- 31.01]\n",
      "   Bi-LSTM: Countries=19, RMSE:    2.868 [   0.169-  29.644], MASE:  13.30 [  1.53- 30.54]\n",
      "   ED-LSTM: Countries=19, RMSE:    2.935 [   0.065-  30.739], MASE:  13.39 [  0.82- 33.22]\n",
      "       CNN: Countries=19, RMSE:    2.823 [   0.148-  28.970], MASE:  13.26 [  2.24- 29.67]\n"
     ]
    }
   ],
   "source": [
    "summary_df = analyse_results(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93370650",
   "metadata": {},
   "source": [
    "### Plot models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a078974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparisons(all_results, target_var, country, save_plots=True, save_dir='data_export'):\n",
    "    \n",
    "    country_results = all_results[target_var][country]\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_models = len(country_results)\n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 4*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    model_names = list(country_results.keys())\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        ax = axes[i]\n",
    "        results = country_results[model_name]\n",
    "        actual = results['actual']\n",
    "        preds = results['predictions']\n",
    "        test_years = results['test_years']\n",
    "        scale_info = results['scale_info']\n",
    "        \n",
    "        ax.plot(test_years, actual, label='Actual', color='blue', alpha=0.7, linewidth=2, marker='o', markersize=4)\n",
    "        ax.plot(test_years, preds, label='Prediction', color='red', alpha=0.7, linewidth=2, marker='s', markersize=3)\n",
    "        \n",
    "        # In scaled units\n",
    "        title = f'{model_name}\\n'\n",
    "        title += f'RMSE: {results[\"rmse\"]:.2f} ({scale_info[\"unit\"]})\\n'\n",
    "        title += f'MASE: {results[\"mase\"]:.2f}'\n",
    "        \n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_xlabel('Year', fontsize=9)\n",
    "        ax.set_ylabel(f'{target_var}\\n({scale_info['unit']})', fontsize=9)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # Hide extra plots\n",
    "    for i in range(len(model_names), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'{country} - {target_var} - Model Comparison for test set', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    if save_plots:\n",
    "        plots_dir = os.path.join(save_dir, 'plots')\n",
    "        if not os.path.exists(plots_dir):\n",
    "            os.makedirs(plots_dir)\n",
    "        \n",
    "        filename = os.path.join(plots_dir, f'{country}_{target_var}_comparison.png')\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved: {filename}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c3ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_countries = ['United States', 'China', 'Germany']\n",
    "sample_targets = ['co2']\n",
    "\n",
    "for target_var in sample_targets:\n",
    "    if target_var in all_results:\n",
    "        for country in sample_countries:\n",
    "            if country in all_results[target_var] and all_results[target_var][country]:\n",
    "                print(f\"Plotting {country} - {target_var}\")\n",
    "                plot_model_comparisons(all_results, target_var, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7949f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_countries = ['United States', 'China', 'Australia']\n",
    "sample_targets = ['gdp']\n",
    "\n",
    "for target_var in sample_targets:\n",
    "    if target_var in all_results:\n",
    "        for country in sample_countries:\n",
    "            if country in all_results[target_var] and all_results[target_var][country]:\n",
    "                print(f\"Plotting {country} - {target_var}\")\n",
    "                plot_model_comparisons(all_results, target_var, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_countries = ['United States', 'China', 'South Korea']\n",
    "sample_targets = ['primary_energy_consumption']\n",
    "\n",
    "for target_var in sample_targets:\n",
    "    if target_var in all_results:\n",
    "        for country in sample_countries:\n",
    "            if country in all_results[target_var] and all_results[target_var][country]:\n",
    "                print(f\"Plotting {country} - {target_var}\")\n",
    "                plot_model_comparisons(all_results, target_var, country)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
