{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ec8cc0",
   "metadata": {},
   "source": [
    "# Step 2-3 ARIMAX\n",
    "\n",
    "## Comparison of ARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f117aa5",
   "metadata": {},
   "source": [
    "ARIMA is autoregressive.\n",
    "\n",
    "ARIMAX can handle exogenous variables, \n",
    "\n",
    "Two experiements for ARIMAX\n",
    "\n",
    "1. energy mix + core features\n",
    "\n",
    "2. energy mix + HHI + core features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02757591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0467d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "TARGET_VARIABLES = 'co2'\n",
    "CANDIDATE_FEATURES = ['gdp', 'primary_energy_consumption']\n",
    "SELECTED_COUNTRIES = ['United States', 'China', 'India']\n",
    "MAX_LAGS = 4\n",
    "TEST_SIZE = 9\n",
    "\n",
    "CORE_FEATURES = ['population']\n",
    "\n",
    "ARIMA_ORDERS = {\n",
    "    'United States': (0, 1, 0),\n",
    "    'China': (0, 2, 0),\n",
    "    'India': (1, 1, 1)\n",
    "}\n",
    "\n",
    "# Important features to include\n",
    "IMPORTANT_FEATURES = [\n",
    "    'population',\n",
    "    'gdp',\n",
    "    'primary_energy_consumption'\n",
    "    #'Proportions', # These are below\n",
    "    #'Energy_mix' \n",
    "]\n",
    "\n",
    "HHI_FEATURES = [\n",
    "    'hhi_detailed',\n",
    "    'hhi_fossil_comp'\n",
    "]\n",
    "\n",
    "PRODUCTION_FEATURES = [\n",
    "    'coal_production',\n",
    "    'oil_production',\n",
    "    'gas_production'\n",
    "]\n",
    "\n",
    "CONSUMPTION_FEATURES = [\n",
    "    'coal_consumption',\n",
    "    'oil_consumption',\n",
    "    'gas_consumption',\n",
    "    'nuclear_consumption',\n",
    "    'biofuel_consumption',\n",
    "    'solar_consumption',\n",
    "    'wind_consumption',\n",
    "    'hydro_consumption',\n",
    "    'other_renewables_consumption'\n",
    "]\n",
    "\n",
    "SHARE_FEATURES = [\n",
    "    'coal_share_energy',\n",
    "    'oil_share_energy',\n",
    "    'gas_share_energy',\n",
    "    'nuclear_share_energy',\n",
    "    'biofuel_share_energy',\n",
    "    'solar_share_energy',\n",
    "    'wind_share_energy',\n",
    "    'hydro_share_energy',\n",
    "    'other_renewables_share_energy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bae542db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase(y_actual, y_pred, period=1):\n",
    "    mae_forecast = mean_absolute_error(y_actual, y_pred)\n",
    "\n",
    "    naive_forecast = y_actual[:-period] if period > 0 else y_actual[:-1]\n",
    "    actual_for_naive = y_actual[period:] if period > 0 else y_actual[1:]\n",
    "\n",
    "    if len(naive_forecast) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    mae_naive = mean_absolute_error(actual_for_naive, naive_forecast)\n",
    "\n",
    "    if mae_naive == 0:\n",
    "        return 0 if mae_forecast == 0 else np.inf\n",
    "    \n",
    "    return mae_forecast / mae_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e9969c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(save_dir='data'):\n",
    "    data_files = {\n",
    "        'train_3_hhi_detail_fossil': os.path.join(save_dir, 'train_3_hhi_detail_fossil.csv'),\n",
    "        'test_3_hhi_detail_fossil': os.path.join(save_dir, 'test_3_hhi_detail_fossil.csv'),\n",
    "        'all_data_df': os.path.join(save_dir, 'all_data_df.csv')\n",
    "    }\n",
    "\n",
    "    dfs = {}\n",
    "    for name, filepath in data_files.items():\n",
    "        if os.path.exists(filepath):\n",
    "            dfs[name] = pd.read_csv(filepath)\n",
    "            print(f\"Loaded {name}: {dfs[name].shape}\")\n",
    "        else:\n",
    "            print(f\"{filepath} not found\")\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eda2e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_3_hhi_detail_fossil: (135, 1002)\n",
      "Loaded test_3_hhi_detail_fossil: (27, 1002)\n",
      "Loaded all_data_df: (55529, 200)\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "train_3_hhi_detail_fossil_df = data['train_3_hhi_detail_fossil']\n",
    "test_3_hhi_detail_fossil_df = data['test_3_hhi_detail_fossil']\n",
    "all_data_df = data['all_data_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20b7d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pct_change(df, feature_cat, max_lags=MAX_LAGS):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sort_values(['country', 'year']).reset_index(drop=True)\n",
    "    pct_change_cols = []\n",
    "\n",
    "    for feature in feature_cat:\n",
    "        if feature not in df_copy.columns:\n",
    "            continue\n",
    "\n",
    "        # Pct change on current values\n",
    "        lag1_col = f\"{feature}_lag1\"\n",
    "        if lag1_col in df_copy.columns:\n",
    "            df_copy[f\"{feature}_pct_change\"] = ((df_copy[feature] - df_copy[lag1_col]) / df_copy[lag1_col] * 100)\n",
    "            pct_change_cols.append(f\"{feature}_pct_change\")\n",
    "\n",
    "        # Pct change on lagged values\n",
    "        for lag in range(1, max_lags):\n",
    "            lag_col = f\"{feature}_lag{lag}\"\n",
    "            prev_lag_col = f\"{feature}_lag{lag+1}\"\n",
    "            \n",
    "            if lag_col in df_copy.columns and prev_lag_col in df_copy.columns:\n",
    "                df_copy[f\"{lag_col}_pct_change\"] = ((df_copy[lag_col] - df_copy[prev_lag_col]) / df_copy[prev_lag_col] * 100)\n",
    "                pct_change_cols.append(f\"{lag_col}_pct_change\")\n",
    "\n",
    "        # Lag4 for the first row = 0, then shift lag3_pct by country\n",
    "        last_lag_col = f\"{feature}_lag{max_lags}\"\n",
    "        lag3_pct_col = f\"{feature}_lag{max_lags-1}_pct_change\"\n",
    "\n",
    "        if last_lag_col in df_copy.columns and lag3_pct_col in df_copy.columns:\n",
    "            df_copy[f\"{last_lag_col}_pct_change\"] = df_copy.groupby('country')[lag3_pct_col].shift(1).fillna(0)\n",
    "            pct_change_cols.append(f\"{last_lag_col}_pct_change\")\n",
    "            \n",
    "        df_copy = df_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df_copy, pct_change_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64fe98dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train years: 1969 - 2013\n",
      "Test years: 2014 - 2022\n"
     ]
    }
   ],
   "source": [
    "train_df = train_3_hhi_detail_fossil_df\n",
    "test_df = test_3_hhi_detail_fossil_df\n",
    "\n",
    "print(f\"Train years: {train_df['year'].min()} - {train_df['year'].max()}\")\n",
    "print(f\"Test years: {test_df['year'].min()} - {test_df['year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f76fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cons_pct, cons_pct_cols = calculate_pct_change(train_df, CONSUMPTION_FEATURES)\n",
    "train_share_pct, share_pct_cols = calculate_pct_change(train_df, SHARE_FEATURES)\n",
    "\n",
    "test_cons_pct, _ = calculate_pct_change(test_df, CONSUMPTION_FEATURES)\n",
    "test_share_pct, _ = calculate_pct_change(test_df, SHARE_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444954b",
   "metadata": {},
   "source": [
    "## ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80c7fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ARIMAX(train_country, test_country, exog_features, order):\n",
    "    train_sorted = train_country.sort_values('year').reset_index(drop=True)\n",
    "    test_sorted = test_country.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "    y_train = train_sorted[TARGET_VARIABLES].values\n",
    "    y_test = test_sorted[TARGET_VARIABLES].values\n",
    "\n",
    "    available_features = [f for f in exog_features if f in train_sorted.columns]\n",
    "    \n",
    "    if not available_features:\n",
    "        return None\n",
    "    \n",
    "    X_train = train_sorted[available_features].values\n",
    "    X_test = test_sorted[available_features].values\n",
    "    \n",
    "    train_mask = ~np.isnan(X_train).any(axis=1) & ~np.isnan(y_train)\n",
    "    test_mask = ~np.isnan(X_test).any(axis=1) & ~np.isnan(y_test)\n",
    "    \n",
    "    X_train = X_train[train_mask]\n",
    "    y_train = y_train[train_mask]\n",
    "    X_test = X_test[test_mask]\n",
    "    y_test = y_test[test_mask]\n",
    "    \n",
    "    if len(y_train) == 0 or len(y_test) == 0:\n",
    "        return None\n",
    "    \n",
    "    model = SARIMAX(y_train, exog=X_train, order=order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    fitted_model = model.fit(disp=False)\n",
    "\n",
    "    forecast = fitted_model.forecast(steps=len(y_test), exog=X_test)\n",
    "\n",
    "    rmse_score = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "    mase_score = mase(y_test, forecast)\n",
    "\n",
    "    return {\n",
    "        'forecast': forecast,\n",
    "        'actual': y_test,\n",
    "        'RMSE': rmse_score,\n",
    "        'MASE': mase_score,\n",
    "        'n_features': len(available_features),\n",
    "        'features': available_features\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68210940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current values, not lags\n",
    "cons_pct_current = [col for col in cons_pct_cols if '_lag' not in col]\n",
    "share_pct_current = [col for col in share_pct_cols if '_lag' not in col]\n",
    "\n",
    "# Combination of feature categories\n",
    "feature_combs = {\n",
    "    'Important': IMPORTANT_FEATURES,\n",
    "    'cons_pct + important': cons_pct_current + IMPORTANT_FEATURES,\n",
    "    'cons_pct + important + HHI': cons_pct_current + IMPORTANT_FEATURES + HHI_FEATURES,\n",
    "    'share_pct + important': share_pct_current + IMPORTANT_FEATURES,\n",
    "    'share_pct + important + HHI': share_pct_current + IMPORTANT_FEATURES + HHI_FEATURES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5aecd63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNITED STATES\n",
      "\n",
      "Training Important\n",
      "Using original dataset\n",
      "    RMSE: 327.7108, MASE: 1.5094, Features: 3\n",
      "\n",
      "Training cons_pct + important\n",
      "Using cons_pct dataset\n",
      "    RMSE: 333.0211, MASE: 1.5071, Features: 11\n",
      "\n",
      "Training cons_pct + important + HHI\n",
      "Using cons_pct dataset\n",
      "    RMSE: 65.1721, MASE: 0.2883, Features: 13\n",
      "\n",
      "Training share_pct + important\n",
      "Using share_pct dataset\n",
      "    RMSE: 308.9255, MASE: 1.3554, Features: 12\n",
      "\n",
      "Training share_pct + important + HHI\n",
      "Using share_pct dataset\n",
      "    RMSE: 92.5064, MASE: 0.4185, Features: 14\n",
      "\n",
      "CHINA\n",
      "\n",
      "Training Important\n",
      "Using original dataset\n",
      "    RMSE: 89.9612, MASE: 0.2545, Features: 3\n",
      "\n",
      "Training cons_pct + important\n",
      "Using cons_pct dataset\n",
      "    RMSE: 3047.1364, MASE: 11.3003, Features: 11\n",
      "\n",
      "Training cons_pct + important + HHI\n",
      "Using cons_pct dataset\n",
      "    RMSE: 3047.1344, MASE: 11.3003, Features: 13\n",
      "\n",
      "Training share_pct + important\n",
      "Using share_pct dataset\n",
      "    RMSE: 3785.7044, MASE: 10.0601, Features: 12\n",
      "\n",
      "Training share_pct + important + HHI\n",
      "Using share_pct dataset\n",
      "    RMSE: 3785.7069, MASE: 10.0601, Features: 14\n",
      "\n",
      "INDIA\n",
      "\n",
      "Training Important\n",
      "Using original dataset\n",
      "    RMSE: 91.4837, MASE: 0.5542, Features: 3\n",
      "\n",
      "Training cons_pct + important\n",
      "Using cons_pct dataset\n",
      "    RMSE: 668.5136, MASE: 4.5283, Features: 11\n",
      "\n",
      "Training cons_pct + important + HHI\n",
      "Using cons_pct dataset\n",
      "    RMSE: 208.8309, MASE: 1.4076, Features: 13\n",
      "\n",
      "Training share_pct + important\n",
      "Using share_pct dataset\n",
      "    RMSE: 975.8227, MASE: 6.7397, Features: 12\n",
      "\n",
      "Training share_pct + important + HHI\n",
      "Using share_pct dataset\n",
      "    RMSE: 975.7675, MASE: 6.7393, Features: 14\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for country in SELECTED_COUNTRIES:\n",
    "    print(f\"\\n{country.upper()}\")\n",
    "    country_results = {}\n",
    "\n",
    "    # Using optimal orders from ARIMA testing\n",
    "    order = ARIMA_ORDERS[country]\n",
    "\n",
    "    for comb_name, features in feature_combs.items():\n",
    "        print(f\"\\nTraining {comb_name}\")\n",
    "\n",
    "        if 'cons_pct' in comb_name:\n",
    "            train_country = train_cons_pct[train_cons_pct['country'] == country]\n",
    "            test_country = test_cons_pct[test_cons_pct['country'] == country]\n",
    "            print(f\"Using cons_pct dataset\")\n",
    "        elif 'share_pct' in comb_name:\n",
    "            train_country = train_share_pct[train_share_pct['country'] == country]\n",
    "            test_country = test_share_pct[test_cons_pct['country'] == country]\n",
    "            print(f\"Using share_pct dataset\")\n",
    "        else:\n",
    "            train_country = train_df[train_df['country'] == country]\n",
    "            test_country = test_df[test_df['country'] == country]\n",
    "            print(f\"Using original dataset\")\n",
    "\n",
    "        result = train_ARIMAX(train_country, test_country, features, order)\n",
    "\n",
    "        if result:\n",
    "            country_results[comb_name] = result\n",
    "            print(f\"    RMSE: {result['RMSE']:.4f}, MASE: {result['MASE']:.4f}, Features: {result['n_features']}\")\n",
    "        else:\n",
    "            print(f\"    Failed to train\")\n",
    "    \n",
    "    all_results[country] = country_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fa363df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RMSE by Feature Combination and Country:\n",
      "Country                      United States    China   India\n",
      "Feature_Combo                                              \n",
      "Important                           327.71    89.96   91.48\n",
      "cons_pct + important                333.02  3047.14  668.51\n",
      "cons_pct + important + HHI           65.17  3047.13  208.83\n",
      "share_pct + important               308.93  3785.70  975.82\n",
      "share_pct + important + HHI          92.51  3785.71  975.77\n"
     ]
    }
   ],
   "source": [
    "summary_data = []\n",
    "\n",
    "for country in SELECTED_COUNTRIES:\n",
    "    for combo_name, result in all_results[country].items():\n",
    "        summary_data.append({\n",
    "            'Country': country,\n",
    "            'Feature_Combo': combo_name,\n",
    "            'RMSE': result['RMSE'],\n",
    "            'MASE': result['MASE'],\n",
    "            'N_Features': result['n_features']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "rmse_pivot = summary_df.pivot(index='Feature_Combo', columns='Country', values='RMSE')\n",
    "rmse_pivot = rmse_pivot.round(2)\n",
    "rmse_pivot = rmse_pivot[SELECTED_COUNTRIES]\n",
    "\n",
    "print(\"\\n\\nRMSE by Feature Combination and Country:\")\n",
    "print(rmse_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe89fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11fb05b1",
   "metadata": {},
   "source": [
    "Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d229f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
